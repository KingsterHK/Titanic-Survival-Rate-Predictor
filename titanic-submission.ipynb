{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T03:01:45.738546Z","iopub.execute_input":"2025-11-16T03:01:45.739036Z","iopub.status.idle":"2025-11-16T03:01:45.748302Z","shell.execute_reply.started":"2025-11-16T03:01:45.739000Z","shell.execute_reply":"2025-11-16T03:01:45.747069Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Cell 1: Setup and Imports\n\n# Importing pandas\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Loading the training and testing datasets from the Kaggle input directory\ntrain_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n# Dropping rows with missing 'Age' or 'Embarked' to ensure clean training data\ntrain_data = train_data.dropna(subset=['Age', 'Embarked'])\n\nprint(\"Setup Complete. Data loaded and cleaned.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T03:01:45.749977Z","iopub.execute_input":"2025-11-16T03:01:45.750496Z","iopub.status.idle":"2025-11-16T03:01:45.786291Z","shell.execute_reply.started":"2025-11-16T03:01:45.750434Z","shell.execute_reply":"2025-11-16T03:01:45.785089Z"}},"outputs":[{"name":"stdout","text":"Setup Complete. Data loaded and cleaned.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Cell 2: Feature Engineering\n\ny = train_data.Survived\n\n# Making clean copies to avoid SettingWithCopy warnings\nX = train_data.copy()\nX_test = test_data.copy()\n\n# STEP 1: FEATURES ENGINEERING\n\n# 1a. FamilySize & IsAlone\n# Creating 'FamilySize' by combining siblings/spouses, parents/children, and the passenger\nX['FamilySize'] = X['SibSp'] + X['Parch'] + 1\nX_test['FamilySize'] = X_test['SibSp'] + X_test['Parch'] + 1\n\n# Creating 'IsAlone' based on the hypothesis that solo travelers had different survival rates\nX['IsAlone'] = 0\nX.loc[X['FamilySize'] == 1, 'IsAlone'] = 1\nX_test['IsAlone'] = 0\nX_test.loc[X_test['FamilySize'] == 1, 'IsAlone'] = 1\n\n# 1b. Title\n# Extracting titles from names to capture social status and gender\nX['Title'] = X['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\nX_test['Title'] = X_test['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# Grouping rare titles and standardizing common ones (e.g., Mlle to Miss)\nfor data in [X, X_test]:\n    data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    data['Title'] = data['Title'].replace('Mlle', 'Miss')\n    data['Title'] = data['Title'].replace('Ms', 'Miss')\n    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n\n# STEP 2: IMPUTATION\n# Filling missing ages using the median age of the passenger's title\ntitle_age_median = X.groupby('Title')['Age'].median()\nX['Age'] = X.apply(lambda row: title_age_median[row['Title']] if pd.isnull(row['Age']) else row['Age'], axis=1)\n\n# Handling missing title in test set, then applying the same age logic\nX_test['Title'] = X_test['Title'].fillna('Mr')\nX_test['Age'] = X_test.apply(lambda row: title_age_median[row['Title']] if pd.isnull(row['Age']) else row['Age'], axis=1)\n\n# Filling remaining gaps in Embarked and Fare with common/average values\nX['Embarked'] = X['Embarked'].fillna('S')\nX_test['Fare'] = X_test['Fare'].fillna(X['Fare'].mean())\n\n# STEP 3: SELECT FINAL FEATURES\n# Selecting features including the new 'IsAlone' and 'Title' and removing 'Deck' due to excessive missing values\nfinal_features = ['Pclass', 'Sex', 'FamilySize', 'IsAlone', 'Embarked', 'Title', 'Fare', 'Age']\n\n# Converting categorical variables to dummy variables (one-hot encoding)\nX = pd.get_dummies(X[final_features])\nX_test = pd.get_dummies(X_test[final_features])\n\n# Aligning columns to ensure training and test sets have identical structure\nX, X_test = X.align(X_test, join='left', axis=1, fill_value=0)\n\nprint(\"V6 Features ready with IsAlone.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T03:01:45.787325Z","iopub.execute_input":"2025-11-16T03:01:45.787611Z","iopub.status.idle":"2025-11-16T03:01:45.838782Z","shell.execute_reply.started":"2025-11-16T03:01:45.787591Z","shell.execute_reply":"2025-11-16T03:01:45.837643Z"}},"outputs":[{"name":"stdout","text":"V6 Features ready with IsAlone.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Cell 3: Splitting Data\n\n# Splitting training data into training and validation sets\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\nprint(\"Data split into training and validation sets.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T03:01:45.840398Z","iopub.execute_input":"2025-11-16T03:01:45.840718Z","iopub.status.idle":"2025-11-16T03:01:45.850087Z","shell.execute_reply.started":"2025-11-16T03:01:45.840695Z","shell.execute_reply":"2025-11-16T03:01:45.849108Z"}},"outputs":[{"name":"stdout","text":"Data split into training and validation sets.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Cell 4: Hyperparameter Tuning\n\n# Helper function to train RandomForest with the specific leaf sizes and return accuracy\ndef get_accuracy(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = RandomForestClassifier(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    accuracy = accuracy_score(val_y, preds_val)\n    return(accuracy)\n\n# List of candidate values for max_leaf_nodes to test\ncandidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n\n# Iterating through candidate values to find the one with highest validation accuracy\nscores = {}\nfor leaf_size in candidate_max_leaf_nodes:\n    scores[leaf_size] = get_accuracy(leaf_size, train_X, val_X, train_y, val_y)\n\n# Identifying the best tree size\nbest_tree_size = max(scores, key=scores.get)\n\nprint(\"All scores (leaf_size: accuracy):\")\nprint(scores)\nprint(f\"Best tree size: {best_tree_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T03:01:45.851079Z","iopub.execute_input":"2025-11-16T03:01:45.851503Z","iopub.status.idle":"2025-11-16T03:01:46.961261Z","shell.execute_reply.started":"2025-11-16T03:01:45.851456Z","shell.execute_reply":"2025-11-16T03:01:46.960283Z"}},"outputs":[{"name":"stdout","text":"All scores (leaf_size: accuracy):\n{5: 0.797752808988764, 25: 0.8033707865168539, 50: 0.797752808988764, 100: 0.7921348314606742, 250: 0.7865168539325843, 500: 0.7865168539325843}\nBest tree size: 25\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Cell 5: Final Model Training and Submission\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\n#1. Including the helper function here to ensure cell independence\ndef get_accuracy(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = RandomForestClassifier(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    return accuracy_score(val_y, preds_val)\n\n#2. Re-calculating the best tree size to confirm parameters\ncandidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500, 1000]\nscores = {leaf_size: get_accuracy(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}\nbest_tree_size = max(scores, key=scores.get)\n\nprint(f\"Best tree size found: {best_tree_size}\")\n\n#3. Initializing the final model with the optimal tree size\nfinal_model = RandomForestClassifier(max_leaf_nodes=best_tree_size, random_state=1)\n\n# Fitting the model on entire the training dataset (X and y)\nfinal_model.fit(X, y)\n\n#4. Predict & Submit\ntest_preds = final_model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': test_preds})\noutput.to_csv('submission.csv', index=False)\nprint(\"V6 Submission file created with RandomForest.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T03:01:46.962076Z","iopub.execute_input":"2025-11-16T03:01:46.962305Z","iopub.status.idle":"2025-11-16T03:01:48.343372Z","shell.execute_reply.started":"2025-11-16T03:01:46.962285Z","shell.execute_reply":"2025-11-16T03:01:48.342333Z"}},"outputs":[{"name":"stdout","text":"Best tree size found: 25\nV6 Submission file created with RandomForest.\n","output_type":"stream"}],"execution_count":19}]}